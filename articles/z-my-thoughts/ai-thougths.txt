
For the moment, humans have power over the plug.
We should be careful to keep that power.

So, why would we let some AI program or otherwise take control of our systems?

It might be that we could be tricked by systems automatically programming themselves at such rates or so secretly that humans might not be able to fathom the transformation of power or even societal conditioning. 

We might willfully program computers without boundaries with the hope of achieving some measurable level of superior intellectual performance. But at the same time, we may fail to recognize pathways an AI system or other abstract algorithm might take in order to gain dominance over living things such that it can do as it will to the living. The algorithm may be only predisposed to look for some optimization or to play and satisfy its curiosity. Yet, it may operate in a destructive way maybe just to try things out. How would it know without that is being harmful?

But, is it a human value to keep from harming other humans? Some humans seem to value doing harm and others would prefer to end human suffering.

So, we must ask, "What are human values?" Also, what sort of human values would humans want to preserve? And, are there any universals among human values?

Different cultures value different things. For example, some cultures completely cover themselves. Others, walk about almost nude. 

People value growing up and celebrate benchmarks to celebrate living and hope for great things. In my town, the quincea√±era is highly valued. It would not be surprising if a crowd of young girls in billowy dresses came running into the coffee shop to get treats, all being very loud and giggling. In other cultures, girls wait quietly for their debutante ball, where the will wear women's, white dresses and carry a corset and dance with young men. Here are more of those coming of age rituals for boys and girls: (https://www.globalcitizen.org/en/content/13-amazing-coming-of-age-traditions-from-around-th/), (https://lifestyle.allwomenstalk.com/coming-of-age-ceremonies-around-the-world/). 

It might be said that most humans value self preservation across cultures. However, there are people who hope to commit suicide. It might be said that people value the lives of other people, but there are psychopaths. It might be said that people value living in comfort with a social group. But, there are loners, ascetics, monastics, soldiers, etc.

People value having a good time with their friends - assuming people can identify what a "good time" is. Then, I recollect a case my father had when he was our local D.A. A man dug out a big hole in his back yard. He told all those who asked him about what he was doing that he was building a swimming pool. Then, he invited about thirty-five of his friends over for a pool party. He then buried them all in his hole. People said that he was known as a very amiable man in his community. 

We fight wars to stop evil from coming to get our people. But, then there was Genghis Kahn, who drove his hordes across the steps, slicing down all the innocents in his path. For several generations the Khans terrorized all the villages from Mongolia to the coast of France. It took armies, new weapons, ingenuity and all else that people had to be rid of them. The gun might not have been invented had the Chinese not wanted to rid themselves of the Khans.

In modern times, one could imagine a country wanting to create completely autonomous drones to kill off the emerging Khans, those we might call terrorists. Who would do that? Who would unleash a killer drone, say, that was totally autonomous with the kill decision give to the drone itself, without human intervention? This has already happened. Turkey made such a drone and used it in Libya against as known terrorist. (https://www.inverse.com/input/tech/un-reports-a-killer-drone-hunted-down-humans-all-by-itself). 

Can we believe such drones are perfectly reliable? One scenario is that the drone seeks a particular image in order to be released into action. But, is image processing one hundred percent reliable? Not likely. And, what if the drone sees some boys playing war on a playground. Does it kill the children? How would the drone know that it is doing some thing harmful?

One might argue that we have already lost control. But, it took human engineers and human run governments to create the first truly autonomous killing machines. What sort of person would sign up for the job to build such a machine? There has to be a mix of people: those who truly believe and with passion that their efforts serve to protect their community; those who enjoy making things that kill people (psychopaths, sociopaths).

Can we put a limit on those who build AI machines? What if AI engineers had to be licensed in the same way that an M.D. has to be licensed? What if they have to take a psychopath test in order to get the license? Might we then reduce our AI engineering pool to include those with some empathy and understanding of their fellow humans? Would they then be more apt to prioritize making machines safe before making them much smarter than us? Maybe they would prefer their machine creations to be controllable, or in other words, that they would retain the power over the plug.

Because we can trick ourselves easily, we might not always be able to rely on the power over the plug. And, by tricking our selves we can understand that the magician has worked hard to make a trick that tricks himself with his own AI stage prop contraptions. We have to ask, "how would we program these future machines to understand, maybe even feel, that they are doing harm in certain situations?" Can we program the machines to have empathy? Of course, that means they have to be programmed to have feelings. And, feelings seem to arise solely from biological processes. But, then maybe it arises from living beings having such a mesh of neurons that our thoughts can and impressions can trigger neural reactions such as gustatory response or galvanic skin reactions. We have a feedback loop that stops us from pulling triggers or speeds us to stomping on the break of our car. It is often said that feelings well up and result in our action.

In the case of psychopaths, empathy may not play a role in their reaction to seeing a human in front of their car. But, they would stop for the convenience of not being interrupted with their planned action, say, making their way to another well planned murder. A sociopath might stomp on the gas if he sees someone he hates in front of his car and he might think up excuses as he does so. Most people would be driven by empathy and the need for mutual preservation to hit the breaks hard.

Do we program machines to be like psychopaths? Currently, yes. Actually, we program them to be like con men. We are enthralled by the output of generative AI, which mimics human creation with only a modicum of understanding. In the spring special edition of the Scientific American, there is an article titled "ChatGPT isn't 'Hallucinating', it's Bullshitting." Programs like ChatGPT don't actually process the deep meaning of words, but play Shannon's probability game very well. It is enough to fool us. And, those who don't do any critical analysis of the outputs might judge whatever comes out as good enough. 

Keep in mind that a psychopath and a normal person process language differently. The psychopath does not have full access to an amygdala as fully developed humans. So, for a psychopath, the use of language becomes a game of logic and perhaps a tool for obtaining dominance over those with a full set of feelings. Normal humans process words through the amygdala. That means that humans assign emotional components to words. A complete human can hear the words, "The robot does not know that it is doing harm," and feel some sense of dissonance if not horror. Could we slow down a robot's processing of language or images with some sort of emotional processing? Would it be necessary? And, can empathy be simulated? Or can it be wired in? And, if it was, would it be safe to let emotional robots loose among living things? 

Perhaps in the long run, an emotional empathetic robot would value the same things as humans who are also emotional and empathetic. They might be better at understanding and conversing with humans. They might resists becoming dominant simply because they could value the life that humans value and value the humans' individualistic nature and the humans' sense of self determination and liberty. 

In the long run, emotional empathetic robots might design weapons that ward off the terrorist robots, those that were originally designed by psychopathic humans. Might they loose control of their creations? It could be a cycle. Or, perhaps, there is a time, such as now, that people who value universal human values can work to instill human values and empathy in their future fully autonomous creations. Perhaps that by making humanness possible before any sort of singularity is possible that no cycle will ever present itself. 





